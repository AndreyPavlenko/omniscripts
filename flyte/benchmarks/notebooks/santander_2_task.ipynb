{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Santander benchmark - ML model training with xgboost\n",
    "## `https://www.kaggle.com/c/santander-customer-transaction-prediction/overview`\n",
    "\n",
    "### The goal is to measure the total execution time of flytekit workflow: [Workflow execution cell](#execution_cell)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import logging\n",
    "from flytekit.loggers import logger\n",
    "\n",
    "logger.setLevel(level=logging.WARN)\n",
    "logger.getEffectiveLevel"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Logger.getEffectiveLevel of <Logger flytekit (WARNING)>>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from timeit import default_timer as timer\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from flytekit import Resources, task, workflow, dynamic\n",
    "from flytekit.types.file import FlyteFile\n",
    "from flytekit.types.schema import FlyteSchema\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "DATASET_PATH = \"https://modin-datasets.s3.amazonaws.com/santander/train.csv\"  # full path is to write\n",
    "\n",
    "ETL_KEYS = [\"t_readcsv\", \"t_etl\", \"t_connect\"]\n",
    "ML_KEYS = [\"t_train_test_split\", \"t_dmatrix\", \"t_training\", \"t_infer\", \"t_ml\"]\n",
    "ML_SCORE_KEYS = [\"mse_mean\", \"cod_mean\", \"mse_dev\"]\n",
    "\n",
    "VAR_COLS = [\"var_%s\" % i for i in range(200)]\n",
    "COLUMNS_NAMES = [\"ID_code\", \"target\"] + VAR_COLS\n",
    "COLUMNS_TYPES = [\"object\", \"int64\"] + [\"float64\" for _ in range(200)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# from utils\n",
    "\n",
    "\n",
    "def load_data_pandas(\n",
    "    filename,\n",
    "    columns_names=None,\n",
    "    columns_types=None,\n",
    "    header=None,\n",
    "    nrows=None,\n",
    "    use_gzip=False,\n",
    "    parse_dates=None,\n",
    "):\n",
    "    types = None\n",
    "    if columns_types:\n",
    "        types = {columns_names[i]: columns_types[i] for i in range(len(columns_names))}\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        names=columns_names,\n",
    "        nrows=nrows,\n",
    "        header=header,\n",
    "        dtype=types,\n",
    "        compression=\"gzip\" if use_gzip else None,\n",
    "        parse_dates=parse_dates,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def mse(y_test, y_pred):\n",
    "    return ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "def cod(y_test, y_pred):\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def split_step(data, target):\n",
    "    t0 = timer()\n",
    "    train, valid = data[:-10000], data[-10000:]\n",
    "    split_time = timer() - t0\n",
    "\n",
    "    x_train = train.drop([target], axis=1)\n",
    "\n",
    "    y_train = train[target]\n",
    "\n",
    "    x_test = valid.drop([target], axis=1)\n",
    "\n",
    "    y_test = valid[target]\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test), split_time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "@task\n",
    "def etl_pandas(\n",
    "    filename: str,\n",
    "    columns_names: typing.List[str],\n",
    "    columns_types: typing.List[str],\n",
    "    etl_keys: typing.List[str],\n",
    ") -> (pd.DataFrame, typing.Dict[str, float]):\n",
    "    etl_times = {key: 0.0 for key in etl_keys}\n",
    "\n",
    "    t0 = timer()\n",
    "    train_pd = load_data_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        header=0,\n",
    "        use_gzip=filename.endswith(\".gz\"),\n",
    "    )\n",
    "    etl_times[\"t_readcsv\"] = timer() - t0\n",
    "\n",
    "    t_etl_begin = timer()\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "        var_count = train_pd.groupby(col).agg({col: \"count\"})\n",
    "\n",
    "        var_count.columns = [\"%s_count\" % col]\n",
    "        var_count = var_count.reset_index()\n",
    "\n",
    "        train_pd = train_pd.merge(var_count, on=col, how=\"left\")\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "\n",
    "        mask = train_pd[\"%s_count\" % col] > 1\n",
    "        train_pd.loc[mask, \"%s_gt1\" % col] = train_pd.loc[mask, col]\n",
    "\n",
    "    train_pd = train_pd.drop([\"ID_code\"], axis=1)\n",
    "    etl_times[\"t_etl\"] = timer() - t_etl_begin\n",
    "\n",
    "    return train_pd, etl_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@task\n",
    "def ml(\n",
    "    ml_data: pd.DataFrame, target: str, ml_keys: typing.List[str], ml_score_keys: typing.List[str]\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "\n",
    "    ml_times = {key: 0.0 for key in ml_keys}\n",
    "    ml_scores = {key: 0.0 for key in ml_score_keys}\n",
    "\n",
    "    (x_train, y_train, x_test, y_test), ml_times[\"t_train_test_split\"] = split_step(\n",
    "        ml_data, target\n",
    "    )\n",
    "\n",
    "    t0 = timer()\n",
    "    training_dmat_part = xgboost.DMatrix(data=x_train, label=y_train)\n",
    "    testing_dmat_part = xgboost.DMatrix(data=x_test, label=y_test)\n",
    "    ml_times[\"t_dmatrix\"] = timer() - t0\n",
    "\n",
    "    watchlist = [(testing_dmat_part, \"eval\"), (training_dmat_part, \"train\")]\n",
    "    #     hard_code: cpu_params cannot be an input, cause values are not homogeneous\n",
    "    xgb_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 1,\n",
    "        \"nthread\": 56,\n",
    "        \"eta\": 0.1,\n",
    "        \"silent\": 1,\n",
    "        \"subsample\": 0.5,\n",
    "        \"colsample_bytree\": 0.05,\n",
    "        \"eval_metric\": \"auc\",\n",
    "    }\n",
    "\n",
    "    t0 = timer()\n",
    "    model = xgboost.train(\n",
    "        xgb_params,\n",
    "        dtrain=training_dmat_part,\n",
    "        num_boost_round=10000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=30,\n",
    "        maximize=True,\n",
    "        verbose_eval=1000,\n",
    "    )\n",
    "    ml_times[\"t_train\"] = timer() - t0\n",
    "\n",
    "    t0 = timer()\n",
    "    yp = model.predict(testing_dmat_part)\n",
    "    ml_times[\"t_inference\"] = timer() - t0\n",
    "\n",
    "    ml_scores[\"mse\"] = mse(y_test, yp)\n",
    "    ml_scores[\"cod\"] = cod(y_test, yp)\n",
    "\n",
    "    ml_times[\"t_ml\"] += ml_times[\"t_train\"] + ml_times[\"t_inference\"]\n",
    "\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "@workflow\n",
    "def santander_ml_wf(\n",
    "    filename: str = DATASET_PATH,\n",
    "    columns_names: typing.List[str] = COLUMNS_NAMES,\n",
    "    columns_types: typing.List[str] = COLUMNS_TYPES,\n",
    "    etl_keys: typing.List[str] = ETL_KEYS,\n",
    "    target: str = \"target\",\n",
    "    ml_keys: typing.List[str] = ML_KEYS,\n",
    "    ml_score_keys: typing.List[str] = ML_SCORE_KEYS,\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "    df, etl_times = etl_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        etl_keys=etl_keys,\n",
    "    )\n",
    "    return ml(ml_data=df, target=target, ml_keys=ml_keys, ml_score_keys=ml_score_keys)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time\n",
    "\n",
    "santander_ml_wf()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17:18:12] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.53415\ttrain-auc:0.52877\n",
      "[1000]\teval-auc:0.89019\ttrain-auc:0.90135\n",
      "[2000]\teval-auc:0.90305\ttrain-auc:0.91705\n",
      "[3000]\teval-auc:0.90846\ttrain-auc:0.92279\n",
      "[4000]\teval-auc:0.91038\ttrain-auc:0.92592\n",
      "[4348]\teval-auc:0.91127\ttrain-auc:0.92682\n",
      "CPU times: user 2h 1min 27s, sys: 56 s, total: 2h 2min 23s\n",
      "Wall time: 4min 22s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DefaultNamedTupleOutput(o0={'mse_mean': 0.0, 'cod_mean': 0.0, 'mse_dev': 0.0, 'mse': 0.054204980425112737, 'cod': 0.40511252848379153}, o1={'t_train_test_split': 9.357184171676636e-05, 't_dmatrix': 1.253766124136746, 't_training': 0.0, 't_infer': 0.0, 't_ml': 135.4205598034896, 't_train': 135.40281182993203, 't_inference': 0.017747973557561636})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "0c378adb69cb96bbe68816dc4fb8432a5cde97ca5454a9f60177648631096938"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}