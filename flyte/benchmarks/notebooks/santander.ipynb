{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Santander benchmark - ML model training with xgboost\n",
    "## `https://www.kaggle.com/c/santander-customer-transaction-prediction/overview`\n",
    "\n",
    "### The goal is to measure the total execution time of flytekit workflow:\n",
    "* ### [Workflow execution cell - 2 task division](#execution_cell_1)\n",
    "\n",
    "* ### [Workflow execution cell - more detalized task division](#execution_cell_2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import logging\n",
    "from flytekit.loggers import logger\n",
    "\n",
    "logger.setLevel(level=logging.WARN)\n",
    "logger.getEffectiveLevel"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Logger.getEffectiveLevel of <Logger flytekit (WARNING)>>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from timeit import default_timer as timer\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from flytekit import Resources, task, workflow, dynamic\n",
    "from flytekit.types.file import FlyteFile\n",
    "from flytekit.types.schema import FlyteSchema\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Common part: global variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "DATASET_PATH = \"https://modin-datasets.s3.amazonaws.com/santander/train.csv\"  # full path is to write\n",
    "ETL_KEYS = [\"t_readcsv\", \"t_etl\", \"t_connect\"]\n",
    "ML_KEYS = [\"t_train_test_split\", \"t_dmatrix\", \"t_training\", \"t_infer\", \"t_ml\"]\n",
    "ML_SCORE_KEYS = [\"mse_mean\", \"cod_mean\", \"mse_dev\"]\n",
    "\n",
    "VAR_COLS = [\"var_%s\" % i for i in range(200)]\n",
    "COLUMNS_NAMES = [\"ID_code\", \"target\"] + VAR_COLS\n",
    "COLUMNS_TYPES = [\"object\", \"int64\"] + [\"float64\" for _ in range(200)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Workflow consisting from 2 tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# from utils\n",
    "\n",
    "def load_data_pandas(\n",
    "    filename,\n",
    "    columns_names=None,\n",
    "    columns_types=None,\n",
    "    header=None,\n",
    "    nrows=None,\n",
    "    use_gzip=False,\n",
    "    parse_dates=None,\n",
    "):\n",
    "    types = None\n",
    "    if columns_types:\n",
    "        types = {columns_names[i]: columns_types[i] for i in range(len(columns_names))}\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        names=columns_names,\n",
    "        nrows=nrows,\n",
    "        header=header,\n",
    "        dtype=types,\n",
    "        compression=\"gzip\" if use_gzip else None,\n",
    "        parse_dates=parse_dates,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def mse(y_test, y_pred):\n",
    "    return ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "def cod(y_test, y_pred):\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def split_step(data, target):\n",
    "    t0 = timer()\n",
    "    train, valid = data[:-10000], data[-10000:]\n",
    "    split_time = timer() - t0\n",
    "\n",
    "    x_train = train.drop([target], axis=1)\n",
    "\n",
    "    y_train = train[target]\n",
    "\n",
    "    x_test = valid.drop([target], axis=1)\n",
    "\n",
    "    y_test = valid[target]\n",
    "\n",
    "    return (x_train, y_train, x_test, y_test), split_time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@task\n",
    "def etl_pandas(\n",
    "    filename: str,\n",
    "    columns_names: typing.List[str],\n",
    "    columns_types: typing.List[str],\n",
    "    etl_keys: typing.List[str],\n",
    ") -> (pd.DataFrame, typing.Dict[str, float]):\n",
    "    etl_times = {key: 0.0 for key in etl_keys}\n",
    "\n",
    "    t0 = timer()\n",
    "    train_pd = load_data_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        header=0,\n",
    "        use_gzip=filename.endswith(\".gz\"),\n",
    "    )\n",
    "    etl_times[\"t_readcsv\"] = timer() - t0\n",
    "\n",
    "    t_etl_begin = timer()\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "        var_count = train_pd.groupby(col).agg({col: \"count\"})\n",
    "\n",
    "        var_count.columns = [\"%s_count\" % col]\n",
    "        var_count = var_count.reset_index()\n",
    "\n",
    "        train_pd = train_pd.merge(var_count, on=col, how=\"left\")\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "\n",
    "        mask = train_pd[\"%s_count\" % col] > 1\n",
    "        train_pd.loc[mask, \"%s_gt1\" % col] = train_pd.loc[mask, col]\n",
    "\n",
    "    train_pd = train_pd.drop([\"ID_code\"], axis=1)\n",
    "    etl_times[\"t_etl\"] = timer() - t_etl_begin\n",
    "\n",
    "    return train_pd, etl_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "@task\n",
    "def ml(\n",
    "    ml_data: pd.DataFrame, target: str, ml_keys: typing.List[str], ml_score_keys: typing.List[str]\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "\n",
    "    ml_times = {key: 0.0 for key in ml_keys}\n",
    "    ml_scores = {key: 0.0 for key in ml_score_keys}\n",
    "\n",
    "    (x_train, y_train, x_test, y_test), ml_times[\"t_train_test_split\"] = split_step(\n",
    "        ml_data, target\n",
    "    )\n",
    "\n",
    "    t0 = timer()\n",
    "    training_dmat_part = xgboost.DMatrix(data=x_train, label=y_train)\n",
    "    testing_dmat_part = xgboost.DMatrix(data=x_test, label=y_test)\n",
    "    ml_times[\"t_dmatrix\"] = timer() - t0\n",
    "\n",
    "    watchlist = [(testing_dmat_part, \"eval\"), (training_dmat_part, \"train\")]\n",
    "    #     hard_code: cpu_params cannot be an input, cause values are not homogeneous\n",
    "    xgb_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 1,\n",
    "        \"nthread\": 56,\n",
    "        \"eta\": 0.1,\n",
    "        \"silent\": 1,\n",
    "        \"subsample\": 0.5,\n",
    "        \"colsample_bytree\": 0.05,\n",
    "        \"eval_metric\": \"auc\",\n",
    "    }\n",
    "\n",
    "    t0 = timer()\n",
    "    model = xgboost.train(\n",
    "        xgb_params,\n",
    "        dtrain=training_dmat_part,\n",
    "        num_boost_round=10000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=30,\n",
    "        maximize=True,\n",
    "        verbose_eval=1000,\n",
    "    )\n",
    "    ml_times[\"t_train\"] = timer() - t0\n",
    "\n",
    "    t0 = timer()\n",
    "    yp = model.predict(testing_dmat_part)\n",
    "    ml_times[\"t_inference\"] = timer() - t0\n",
    "\n",
    "    ml_scores[\"mse\"] = mse(y_test, yp)\n",
    "    ml_scores[\"cod\"] = cod(y_test, yp)\n",
    "\n",
    "    ml_times[\"t_ml\"] += ml_times[\"t_train\"] + ml_times[\"t_inference\"]\n",
    "\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "@workflow\n",
    "def santander_ml_wf(\n",
    "    filename: str = DATASET_PATH,\n",
    "    columns_names: typing.List[str] = COLUMNS_NAMES,\n",
    "    columns_types: typing.List[str] = COLUMNS_TYPES,\n",
    "    etl_keys: typing.List[str] = ETL_KEYS,\n",
    "    target: str = \"target\",\n",
    "    ml_keys: typing.List[str] = ML_KEYS,\n",
    "    ml_score_keys: typing.List[str] = ML_SCORE_KEYS,\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "    df, etl_times = etl_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        etl_keys=etl_keys,\n",
    "    )\n",
    "    return ml(ml_data=df, target=target, ml_keys=ml_keys, ml_score_keys=ml_score_keys)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='execution_cell_1'>Workflow execution - 2 task division</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%%time\n",
    "\n",
    "santander_ml_wf()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17:25:35] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.53415\ttrain-auc:0.52877\n",
      "[1000]\teval-auc:0.89019\ttrain-auc:0.90135\n",
      "[2000]\teval-auc:0.90305\ttrain-auc:0.91705\n",
      "[3000]\teval-auc:0.90846\ttrain-auc:0.92279\n",
      "[4000]\teval-auc:0.91038\ttrain-auc:0.92592\n",
      "[4347]\teval-auc:0.91130\ttrain-auc:0.92684\n",
      "CPU times: user 2h 1min 57s, sys: 56.6 s, total: 2h 2min 54s\n",
      "Wall time: 4min 13s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DefaultNamedTupleOutput(o0={'mse_mean': 0.0, 'cod_mean': 0.0, 'mse_dev': 0.0, 'mse': 0.054204980425112737, 'cod': 0.40511252848379153}, o1={'t_train_test_split': 9.94708389043808e-05, 't_dmatrix': 1.2653189827688038, 't_training': 0.0, 't_infer': 0.0, 't_ml': 136.55974651966244, 't_train': 136.54124352429062, 't_inference': 0.018502995371818542})"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow consisting from more detalized tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "@task\n",
    "def load_data_pandas(\n",
    "    filename: str,\n",
    "    columns_names: typing.List[str],\n",
    "    columns_types: typing.List[str],\n",
    "    use_gzip: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    types = None\n",
    "    if columns_types:\n",
    "        types = {columns_names[i]: columns_types[i] for i in range(len(columns_names))}\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        names=columns_names,\n",
    "        dtype=types,\n",
    "        header=0,\n",
    "        compression=\"gzip\" if use_gzip else None,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "@task\n",
    "def mse(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    return ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "@task\n",
    "def cod(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "@task\n",
    "def split_step(\n",
    "    data: pd.DataFrame, target: str\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, float):\n",
    "    t0 = timer()\n",
    "    train, valid = data[:-10000], data[-10000:]\n",
    "    split_time = timer() - t0\n",
    "\n",
    "    x_train = train.drop([target], axis=1)\n",
    "\n",
    "    y_train = train[target]\n",
    "\n",
    "    x_test = valid.drop([target], axis=1)\n",
    "\n",
    "    y_test = valid[target]\n",
    "\n",
    "    return x_train, pd.DataFrame(y_train), x_test, pd.DataFrame(y_test), split_time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "@dynamic\n",
    "def etl_pandas(\n",
    "    filename: str,\n",
    "    columns_names: typing.List[str],\n",
    "    columns_types: typing.List[str],\n",
    "    etl_keys: typing.List[str],\n",
    ") -> (pd.DataFrame, typing.Dict[str, float]):\n",
    "    etl_times = {key: 0.0 for key in etl_keys}\n",
    "\n",
    "    t0 = timer()\n",
    "    train_pd = load_data_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        use_gzip=filename.endswith(\".gz\"),\n",
    "    )\n",
    "    etl_times[\"t_readcsv\"] = timer() - t0\n",
    "\n",
    "    t_etl_begin = timer()\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "        var_count = train_pd.groupby(col).agg({col: \"count\"})\n",
    "\n",
    "        var_count.columns = [\"%s_count\" % col]\n",
    "        var_count = var_count.reset_index()\n",
    "\n",
    "        train_pd = train_pd.merge(var_count, on=col, how=\"left\")\n",
    "\n",
    "    for i in range(200):\n",
    "        col = \"var_%d\" % i\n",
    "\n",
    "        mask = train_pd[\"%s_count\" % col] > 1\n",
    "        train_pd.loc[mask, \"%s_gt1\" % col] = train_pd.loc[mask, col]\n",
    "\n",
    "    train_pd = train_pd.drop([\"ID_code\"], axis=1)\n",
    "    etl_times[\"t_etl\"] = timer() - t_etl_begin\n",
    "\n",
    "    return train_pd, etl_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@dynamic\n",
    "def ml(\n",
    "    ml_data: pd.DataFrame, target: str, ml_keys: typing.List[str], ml_score_keys: typing.List[str]\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "\n",
    "    ml_times = {key: 0.0 for key in ml_keys}\n",
    "    ml_scores = {key: 0.0 for key in ml_score_keys}\n",
    "\n",
    "    x_train, y_train, x_test, y_test, ml_times[\"t_train_test_split\"] = split_step(\n",
    "        data=ml_data, target=target\n",
    "    )\n",
    "\n",
    "    t0 = timer()\n",
    "    training_dmat_part = xgboost.DMatrix(data=x_train, label=y_train)\n",
    "    testing_dmat_part = xgboost.DMatrix(data=x_test, label=y_test)\n",
    "    ml_times[\"t_dmatrix\"] = timer() - t0\n",
    "\n",
    "    watchlist = [(testing_dmat_part, \"eval\"), (training_dmat_part, \"train\")]\n",
    "    #     hard_code: cpu_params cannot be an input, cause values are not homogeneous\n",
    "    xgb_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 1,\n",
    "        \"nthread\": 56,\n",
    "        \"eta\": 0.1,\n",
    "        \"silent\": 1,\n",
    "        \"subsample\": 0.5,\n",
    "        \"colsample_bytree\": 0.05,\n",
    "        \"eval_metric\": \"auc\",\n",
    "    }\n",
    "\n",
    "    t0 = timer()\n",
    "    model = xgboost.train(\n",
    "        xgb_params,\n",
    "        dtrain=training_dmat_part,\n",
    "        num_boost_round=10000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=30,\n",
    "        maximize=True,\n",
    "        verbose_eval=1000,\n",
    "    )\n",
    "    ml_times[\"t_train\"] = timer() - t0\n",
    "\n",
    "    t0 = timer()\n",
    "    yp = pd.DataFrame({\"preds\": model.predict(testing_dmat_part)})\n",
    "    ml_times[\"t_inference\"] = timer() - t0\n",
    "\n",
    "    ml_scores[\"mse\"] = mse(y_test=y_test, y_pred=yp)\n",
    "    ml_scores[\"cod\"] = cod(y_test=y_test, y_pred=yp)\n",
    "\n",
    "    ml_times[\"t_ml\"] += ml_times[\"t_train\"] + ml_times[\"t_inference\"]\n",
    "\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "@workflow\n",
    "def santander_ml_wf(\n",
    "    filename: str = DATASET_PATH,\n",
    "    columns_names: typing.List[str] = COLUMNS_NAMES,\n",
    "    columns_types: typing.List[str] = COLUMNS_TYPES,\n",
    "    etl_keys: typing.List[str] = ETL_KEYS,\n",
    "    target: str = \"target\",\n",
    "    ml_keys: typing.List[str] = ML_KEYS,\n",
    "    ml_score_keys: typing.List[str] = ML_SCORE_KEYS,\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "    df, etl_times = etl_pandas(\n",
    "        filename=filename,\n",
    "        columns_names=columns_names,\n",
    "        columns_types=columns_types,\n",
    "        etl_keys=etl_keys,\n",
    "    )\n",
    "    return ml(ml_data=df, target=target, ml_keys=ml_keys, ml_score_keys=ml_score_keys)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='execution_cell_2'>Workflow execution - more detalized task division (subworkflow inclusion)</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time\n",
    "\n",
    "santander_ml_wf()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18:11:27] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.53415\ttrain-auc:0.52877\n",
      "[1000]\teval-auc:0.89019\ttrain-auc:0.90135\n",
      "[2000]\teval-auc:0.90305\ttrain-auc:0.91705\n",
      "[3000]\teval-auc:0.90846\ttrain-auc:0.92279\n",
      "[4000]\teval-auc:0.91038\ttrain-auc:0.92592\n",
      "[4348]\teval-auc:0.91127\ttrain-auc:0.92682\n",
      "CPU times: user 2h 3min 20s, sys: 1min 30s, total: 2h 4min 50s\n",
      "Wall time: 4min 21s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DefaultNamedTupleOutput(o0={'mse_mean': 0.0, 'cod_mean': 0.0, 'mse_dev': 0.0, 'mse': preds    NaN\n",
       "target   NaN\n",
       "dtype: float64, 'cod': preds     NaN\n",
       "target    1.0\n",
       "dtype: float64}, o1={'t_train_test_split': 0.00010265223681926727, 't_dmatrix': 1.907059057150036, 't_training': 0.0, 't_infer': 0.0, 't_ml': 138.34493321506307, 't_train': 138.32563289115205, 't_inference': 0.01930032391101122})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "0c378adb69cb96bbe68816dc4fb8432a5cde97ca5454a9f60177648631096938"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}