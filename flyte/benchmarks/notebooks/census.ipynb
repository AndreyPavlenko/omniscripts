{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Census benchmark\n",
    "## ML workflow\n",
    "\n",
    "### The goal is to measure the total execution time: [Workflow execution cell](#execution_cell)\n",
    "\n",
    "### Dataset link:\n",
    "### `https://rapidsai-data.s3.us-east-2.amazonaws.com/datasets/ipums_education2income_1970-2010.csv.gz`\n",
    "\n",
    "### Competition link:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "from flytekit import Resources, task, workflow, dynamic\n",
    "from flytekit.types.file import FlyteFile\n",
    "from flytekit.types.schema import FlyteSchema\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import config_context"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import logging\n",
    "from flytekit.loggers import logger\n",
    "\n",
    "logger.setLevel(level=logging.WARN)\n",
    "logger.getEffectiveLevel"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Logger.getEffectiveLevel of <Logger flytekit (WARNING)>>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Common part: global variables and functions which don't require @task coverage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def split(X, y, test_size=0.1, stratify=None, random_state=None):\n",
    "    t0 = timer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=stratify, random_state=random_state\n",
    "    )\n",
    "    split_time = timer() - t0\n",
    "\n",
    "    return (X_train, y_train, X_test, y_test), split_time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "DATASET_PATH = \"https://modin-datasets.s3.amazonaws.com/census/ipums_education2income_1970-2010.csv.gz\"\n",
    "\n",
    "COLS = [\n",
    "    \"YEAR\",\n",
    "    \"DATANUM\",\n",
    "    \"SERIAL\",\n",
    "    \"CBSERIAL\",\n",
    "    \"HHWT\",\n",
    "    \"CPI99\",\n",
    "    \"GQ\",\n",
    "    \"PERNUM\",\n",
    "    \"SEX\",\n",
    "    \"AGE\",\n",
    "    \"INCTOT\",\n",
    "    \"EDUC\",\n",
    "    \"EDUCD\",\n",
    "    \"EDUC_HEAD\",\n",
    "    \"EDUC_POP\",\n",
    "    \"EDUC_MOM\",\n",
    "    \"EDUCD_MOM2\",\n",
    "    \"EDUCD_POP2\",\n",
    "    \"INCTOT_MOM\",\n",
    "    \"INCTOT_POP\",\n",
    "    \"INCTOT_MOM2\",\n",
    "    \"INCTOT_POP2\",\n",
    "    \"INCTOT_HEAD\",\n",
    "    \"SEX_HEAD\",\n",
    "]\n",
    "\n",
    "COLUMNS_TYPES = [\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"float\",\n",
    "    \"int\",\n",
    "    \"float\",\n",
    "    \"int\",\n",
    "    \"float\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "    \"float\",\n",
    "]\n",
    "\n",
    "# X = OrderedDict((zip(COLS, list(map(eval, COLUMNS_TYPES)))))\n",
    "# Y = OrderedDict({\"EDUC\": X.pop(\"EDUC\")})\n",
    "# Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# ML specific\n",
    "N_RUNS = 50\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 777\n",
    "\n",
    "ML_KEYS = [\"t_train_test_split\", \"t_train\", \"t_inference\", \"t_ml\"]\n",
    "ML_SCORE_KEYS = [\"mse_mean\", \"cod_mean\", \"mse_dev\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow consisting from 2 tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# utils\n",
    "\n",
    "\n",
    "def mse(y_test, y_pred):\n",
    "    return ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "def cod(y_test, y_pred):\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "@task\n",
    "def feature_eng_task(\n",
    "    data: FlyteFile[typing.TypeVar(\"csv\")], cols: typing.List[str]\n",
    ") -> (pd.DataFrame):\n",
    "\n",
    "    df = pd.read_csv(data)[cols]\n",
    "\n",
    "    df = df[df[\"INCTOT\"] != 9999999]\n",
    "    df = df[df[\"EDUC\"] != -1]\n",
    "    df = df[df[\"EDUCD\"] != -1]\n",
    "\n",
    "    df[\"INCTOT\"] = df[\"INCTOT\"] * df[\"CPI99\"]\n",
    "\n",
    "    for column in cols:\n",
    "        df[column] = df[column].fillna(-1)\n",
    "        df[column] = df[column].astype(\"float64\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@task\n",
    "def ml_task(\n",
    "    df: pd.DataFrame,\n",
    "    random_state: int,\n",
    "    n_runs: int,\n",
    "    test_size: float,\n",
    "    ml_keys: typing.List[str],\n",
    "    ml_score_keys: typing.List[str],\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "\n",
    "    # Fetch the input and output data from train dataset\n",
    "    y = np.ascontiguousarray(df[\"EDUC\"], dtype=np.float64)\n",
    "    X = np.ascontiguousarray(df.drop(columns=[\"EDUC\", \"CPI99\"]), dtype=np.float64)\n",
    "\n",
    "    clf = lm.Ridge()\n",
    "\n",
    "    mse_values, cod_values = [], []\n",
    "    ml_times = {key: 0.0 for key in ml_keys}\n",
    "    ml_scores = {key: 0.0 for key in ml_score_keys}\n",
    "\n",
    "    print(\"ML runs: \", n_runs)\n",
    "    for i in range(n_runs):\n",
    "        (X_train, y_train, X_test, y_test), split_time = split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        ml_times[\"t_train_test_split\"] += split_time\n",
    "        random_state += 777\n",
    "\n",
    "        t0 = timer()\n",
    "        with config_context(assume_finite=True):\n",
    "            model = clf.fit(X_train, y_train)\n",
    "        ml_times[\"t_train\"] += timer() - t0\n",
    "\n",
    "        t0 = timer()\n",
    "        y_pred = model.predict(X_test)\n",
    "        ml_times[\"t_inference\"] += timer() - t0\n",
    "\n",
    "        mse_values.append(mse(y_test, y_pred))\n",
    "        cod_values.append(cod(y_test, y_pred))\n",
    "\n",
    "    ml_times[\"t_ml\"] += ml_times[\"t_train\"] + ml_times[\"t_inference\"]\n",
    "\n",
    "    ml_scores[\"mse_mean\"] = sum(mse_values) / len(mse_values)\n",
    "    ml_scores[\"cod_mean\"] = sum(cod_values) / len(cod_values)\n",
    "    ml_scores[\"mse_dev\"] = pow(\n",
    "        sum([(mse_value - ml_scores[\"mse_mean\"]) ** 2 for mse_value in mse_values])\n",
    "        / (len(mse_values) - 1),\n",
    "        0.5,\n",
    "    )\n",
    "    ml_scores[\"cod_dev\"] = pow(\n",
    "        sum([(cod_value - ml_scores[\"cod_mean\"]) ** 2 for cod_value in cod_values])\n",
    "        / (len(cod_values) - 1),\n",
    "        0.5,\n",
    "    )\n",
    "\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "@workflow\n",
    "def census_bench_wf(\n",
    "    dataset: FlyteFile[\"csv\"] = DATASET_PATH,\n",
    "    cols: typing.List[str] = COLS,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    n_runs: int = N_RUNS,\n",
    "    test_size: float = TEST_SIZE,\n",
    "    ml_keys: typing.List[str] = ML_KEYS,\n",
    "    ml_score_keys: typing.List[str] = ML_SCORE_KEYS,\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "    df = feature_eng_task(data=dataset, cols=cols)\n",
    "    ml_scores, ml_times = ml_task(\n",
    "        df=df,\n",
    "        random_state=random_state,\n",
    "        n_runs=n_runs,\n",
    "        test_size=test_size,\n",
    "        ml_keys=ml_keys,\n",
    "        ml_score_keys=ml_score_keys,\n",
    "    )\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "%%time\n",
    "census_bench_wf()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ML runs:  50\n",
      "CPU times: user 7min 11s, sys: 6min 43s, total: 13min 54s\n",
      "Wall time: 6min 58s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DefaultNamedTupleOutput(o0={'mse_mean': 0.03256456908804994, 'cod_mean': 0.9953675334603814, 'mse_dev': 4.179940420229173e-05, 'cod_dev': 5.869227912341005e-06}, o1={'t_train_test_split': 140.56967029813677, 't_train': 143.43539352389053, 't_inference': 2.4777097539044917, 't_ml': 145.91310327779502})"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow consisting from more detalized tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "FEATURES = OrderedDict((zip(COLS, list(map(eval, COLUMNS_TYPES)))))\n",
    "TARGET = OrderedDict({\"EDUC\": FEATURES.pop(\"EDUC\")})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# utils\n",
    "\n",
    "@task\n",
    "def mse(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    return ((y_test - y_pred) ** 2).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@task\n",
    "def cod(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    y_bar = y_test.mean()\n",
    "    total = ((y_test - y_bar) ** 2).sum()\n",
    "    residuals = ((y_test - y_pred) ** 2).sum()\n",
    "    return 1 - (residuals / total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "@task\n",
    "def feature_eng_task(\n",
    "    data: FlyteFile[typing.TypeVar(\"csv\")], cols: typing.List[str]\n",
    ") -> (pd.DataFrame):\n",
    "\n",
    "    df = pd.read_csv(data)[cols]\n",
    "\n",
    "    df = df[df[\"INCTOT\"] != 9999999]\n",
    "    df = df[df[\"EDUC\"] != -1]\n",
    "    df = df[df[\"EDUCD\"] != -1]\n",
    "\n",
    "    df[\"INCTOT\"] = df[\"INCTOT\"] * df[\"CPI99\"]\n",
    "\n",
    "    for column in cols:\n",
    "        df[column] = df[column].fillna(-1)\n",
    "        df[column] = df[column].astype(\"float64\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "@dynamic\n",
    "def ml_task(\n",
    "    df: pd.DataFrame,\n",
    "    random_state: int,\n",
    "    n_runs: int,\n",
    "    test_size: float,\n",
    "    ml_keys: typing.List[str],\n",
    "    ml_score_keys: typing.List[str],\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "\n",
    "    # Fetch the input and output data from train dataset\n",
    "    #     y = np.ascontiguousarray(df[\"EDUC\"], dtype=np.float64)\n",
    "    #     X = np.ascontiguousarray(df.drop(columns=[\"EDUC\", \"CPI99\"]), dtype=np.float64)\n",
    "    y = df[\"EDUC\"]\n",
    "    X = df.drop(columns=[\"EDUC\", \"CPI99\"])\n",
    "\n",
    "    clf = lm.Ridge()\n",
    "\n",
    "    mse_values, cod_values = [], []\n",
    "    ml_times = {key: 0.0 for key in ml_keys}\n",
    "    ml_scores = {key: 0.0 for key in ml_score_keys}\n",
    "\n",
    "    print(\"ML runs: \", n_runs)\n",
    "    for i in range(n_runs):\n",
    "        (X_train, y_train, X_test, y_test), split_time = split(X=X, y=y)\n",
    "        y_test = pd.DataFrame({\"EDUC\": y_test})\n",
    "        ml_times[\"t_train_test_split\"] += split_time\n",
    "        random_state += 777\n",
    "\n",
    "        t0 = timer()\n",
    "        with config_context(assume_finite=True):\n",
    "            model = clf.fit(X_train, y_train)\n",
    "        ml_times[\"t_train\"] += timer() - t0\n",
    "\n",
    "        t0 = timer()\n",
    "        y_pred = pd.DataFrame({\"EDUC\": model.predict(X_test)})\n",
    "        ml_times[\"t_inference\"] += timer() - t0\n",
    "\n",
    "        mse_values.append(mse(y_test=y_test, y_pred=y_pred))\n",
    "        cod_values.append(cod(y_test=y_test, y_pred=y_pred))\n",
    "\n",
    "    ml_times[\"t_ml\"] += ml_times[\"t_train\"] + ml_times[\"t_inference\"]\n",
    "\n",
    "    ml_scores[\"mse_mean\"] = sum(mse_values) / len(mse_values)\n",
    "    ml_scores[\"cod_mean\"] = sum(cod_values) / len(cod_values)\n",
    "    ml_scores[\"mse_dev\"] = pow(\n",
    "        sum([(mse_value - ml_scores[\"mse_mean\"]) ** 2 for mse_value in mse_values])\n",
    "        / (len(mse_values) - 1),\n",
    "        0.5,\n",
    "    )\n",
    "    ml_scores[\"cod_dev\"] = pow(\n",
    "        sum([(cod_value - ml_scores[\"cod_mean\"]) ** 2 for cod_value in cod_values])\n",
    "        / (len(cod_values) - 1),\n",
    "        0.5,\n",
    "    )\n",
    "\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "@workflow\n",
    "def census_bench_wf(\n",
    "    dataset: FlyteFile[\"csv\"] = DATASET_PATH,\n",
    "    cols: typing.List[str] = COLS,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "    n_runs: int = N_RUNS,\n",
    "    test_size: float = TEST_SIZE,\n",
    "    ml_keys: typing.List[str] = ML_KEYS,\n",
    "    ml_score_keys: typing.List[str] = ML_SCORE_KEYS,\n",
    ") -> (typing.Dict[str, float], typing.Dict[str, float]):\n",
    "    df = feature_eng_task(data=dataset, cols=cols)\n",
    "    ml_scores, ml_times = ml_task(\n",
    "        df=df,\n",
    "        random_state=random_state,\n",
    "        n_runs=n_runs,\n",
    "        test_size=test_size,\n",
    "        ml_keys=ml_keys,\n",
    "        ml_score_keys=ml_score_keys,\n",
    "    )\n",
    "    return ml_scores, ml_times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%time\n",
    "\n",
    "#  how workflow output looks like if ml function is decorated as @dynamic\n",
    "census_bench_wf()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ML runs:  50\n",
      "CPU times: user 10min 17s, sys: 5min 55s, total: 16min 13s\n",
      "Wall time: 11min 44s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DefaultNamedTupleOutput(o0={'mse_mean': EDUC    15.160786\n",
       "dtype: float64, 'cod_mean': EDUC    0.842082\n",
       "dtype: float64, 'mse_dev': EDUC    0.053432\n",
       "dtype: float64, 'cod_dev': EDUC    0.000722\n",
       "dtype: float64}, o1={'t_train_test_split': 361.7690731417388, 't_train': 105.59981523500755, 't_inference': 3.1282578515820205, 't_ml': 108.72807308658957})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "0c378adb69cb96bbe68816dc4fb8432a5cde97ca5454a9f60177648631096938"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}